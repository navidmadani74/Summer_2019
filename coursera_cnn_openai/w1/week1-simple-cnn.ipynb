{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 1.4\n",
    "\n",
    "In the first step I implemented tensorflow a simple cnn using tf 1.4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_utils import *\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 320\n",
    "plt.imshow(x_train[idx])\n",
    "print('y={}'.format(y_train[0][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train/255, x_test/255\n",
    "\n",
    "# build one-hot labels\n",
    "y_train = np.array(list(map(lambda x: np.eye(len(classes))[int(x[0])], y_train.T)))\n",
    "y_test = np.array(list(map(lambda x: np.eye(len(classes))[int(x[0])], y_test.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    W1, W2 = params['w1'], params['w2']\n",
    "    \n",
    "    Z1 = tf.nn.conv2d(X, W1, strides=[1], padding='SAME')\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    P1 = tf.nn.max_pool(A1, ksize=[8], strides=[8], padding='SAME')\n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides=[1], padding='SAME')\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    P2 = tf.nn.max_pool(A2, ksize=[4], strides=[4], padding='SAME')\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, 6, activation_fn=None)\n",
    "    \n",
    "    return Z3\n",
    "\n",
    "\n",
    "def comp_loss(Y, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "                          (logits=Y, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_propagation(X, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batch(X, Y, bs):\n",
    "    permutation = list(np.random.permutation(X.shape[0]))\n",
    "    shuffled_x, shuffled_y = X[permutation,:,:,:], Y[permutation,:]\n",
    "    \n",
    "    batches = []\n",
    "    for k in range(X.shape[0]//bs):\n",
    "        mini_batch_x = shuffled_x[k*bs:(k+1)*bs,:,:,:]\n",
    "        mini_batch_y = shuffled_y[k*bs:(k+1)*bs,:]\n",
    "        batches.append((mini_batch_x, mini_batch_y))\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "epochs = 100\n",
    "with tf.device('/cpu:0'):\n",
    "    bs = 10\n",
    "    \n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, name='X', shape=(None, 64, 64, 3))\n",
    "    Y = tf.placeholder(tf.float32, name='Y', shape=[None, 6])\n",
    "\n",
    "    with tf.variable_scope('declaration', reuse=tf.AUTO_REUSE):\n",
    "        w1 = tf.get_variable('w1', shape=(4,4,3,8), initializer=tf.contrib.layers.xavier_initializer())\n",
    "        w2 = tf.get_variable('w2', shape=(2,2,8,16), initializer=tf.contrib.layers.xavier_initializer())\n",
    "        params = {'w1': w1, 'w2': w2}\n",
    "\n",
    "    output = forward_propagation(X, params)\n",
    "    loss = comp_loss(output, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.005).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_loss = 0\n",
    "            num_batches = x_train.shape[0]//bs\n",
    "            batches = random_mini_batch(x_train, y_train, bs)\n",
    "\n",
    "            for x,y in batches:\n",
    "                _, bloss = sess.run([optimizer, loss], feed_dict={X: x, Y: y})\n",
    "                batch_loss += bloss / num_batches\n",
    "\n",
    "            print(\"loss in epoch {} is : {}\".format(epoch, batch_loss))\n",
    "            saver.save(sess, './sessoin-tmp.cptk')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,  './sessoin-tmp.cptk')\n",
    "    \n",
    "    correct = tf.equal(tf.arg_max(Y,1), tf.arg_max(output,1))\n",
    "    _acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    train_acc = _acc.eval({X:x_train, Y:y_train})\n",
    "    test_acc = _acc.eval({X:x_test, Y:y_test})\n",
    "    \n",
    "    print(\"train_acc: {} test_acc: {}\".format(train_acc, test_acc))\n",
    "    \n",
    "    idx = 12\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    tests = [12, 14, 30, 40]\n",
    "    for idx, i in enumerate(tests):\n",
    "        chosen_x = np.array([x_test[i,:,:,:]])\n",
    "        predicted = tf.arg_max(output, dimension=1)\n",
    "        res = sess.run([predicted], feed_dict={X:chosen_x})\n",
    "        \n",
    "        plt.subplot(1, len(tests), idx+1)\n",
    "        plt.title(\"predicted: {}\".format(res[0]))\n",
    "        plt.imshow(x_test[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2.0\n",
    "\n",
    "In this step i'm gonna implement previous model using keras in tf 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_utils import *\n",
    "\n",
    "x_train, y_train, x_test, y_test, classes = load_dataset()\n",
    "batch_size = 25\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching and shuffling dataset\n",
    "# initial dimentions should match\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "(x_train, y_train)).shuffle(1000).batch(batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "(x_test, y_test)).batch(batch_size)\n",
    "\n",
    "print(train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "\n",
    "\n",
    "class CnnModel(Model):\n",
    "    def __init__(self):\n",
    "        super(CnnModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dense(6, activation='softmax')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "model = CnnModel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
    "test_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='test_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(images)\n",
    "        t_loss = loss(labels, pred)\n",
    "    gradients = tape.gradient(t_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(t_loss)\n",
    "    train_acc(labels, pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  predictions = model(images)\n",
    "  t_loss = loss(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_acc(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for images, labels in test_ds:\n",
    "        test_step(images, labels)\n",
    "\n",
    "    print('Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'.format(\n",
    "        epoch + 1,\n",
    "        train_loss.result(),\n",
    "        train_acc.result(),\n",
    "        test_loss.result(),\n",
    "        test_acc.result()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
